
# Image Captioning Model - BLIP

## Introduction
 Image Captioning Model - BLIP (Bootstrapping Language-Image Pre-training). This model is designed for unified vision-language understanding and generation tasks. It is trained on the COCO (Common Objects in Context) dataset using a base architecture with a ViT (Vision Transformer) large backbone.

The image captioning model is implemented using the PyTorch framework and leverages the Hugging Face Transformers library for efficient natural language processing.

Used streamlit python library for creating interactive web applications.

## Demo
Caption can be generate for any image at [link](https://huggingface.co/spaces/luv-bansal/demo-app)


## Example Images with Generated Captions
Here are some example images along with the captions generated by the BLIP image captioning model:

![Image 1](images/football.jpeg)

**Generated Caption:** "Group of young children playing soccer on a field."

![Image 2](images/jeep-woods.jpg)

**Generated Caption:** "Jeep driving down a dirt road in the woods."

![Image 3](images/sunset.jpeg)

**Generated Caption:** "A beautiful sunset over the ocean with waves crashing on the shore."



## Requirements
To run the image captioning model, the following dependencies are required:
- Python (version 3.7 or above)
- PyTorch (version 1.8 or above)
- Transformers library (version 4.3 or above)

You can install the necessary libraries using the following command:

```
pip install -r requirements.txt
```

## Acknowledgments
We would like to express our gratitude to the researchers and developers who have contributed to the development and implementation of the BLIP image captioning model. Their dedication and hard work are greatly appreciated.

## Contact
If you have any questions, issues, or feedback regarding the image captioning model, please feel free to contact us at [bansal22luvi@gmail.com](mailto:bansal22luvi@gmail.com).

We hope you find the BLIP image captioning model useful and enjoy experimenting with it!

